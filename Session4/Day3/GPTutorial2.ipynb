{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division, absolute_import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Process regression tutorial 2:\n",
    "\n",
    "In this tutorial, we are to explore some slightly more realistic applications of GPs to astrophysical (or at least, astronomy-like) datasets. We will do this using the popular `george` package by Daniel Foreman-Mackey.\n",
    "\n",
    "* * *\n",
    "\n",
    "By S Aigrain (University of Oxford)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we are to explore some slightly more realistic applications of GPs to astrophysical (or at least, astronomy-like) datasets. \n",
    "\n",
    "We will do this using the popular `george` package by Daniel Foreman-Mackey. `george` doesn't have all the functionality of more general packages such as `GPy` and `scikit-learn`, but it still has a nice modelling interface, is easy to use, and is faster than either of the other two. \n",
    "\n",
    "We will also use another of Dan's packages, `emcee` to explore posterior probabilities using MCMC, and his `corner.py` module to plot the resulting parameter samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required packages\n",
    "\n",
    "**Why `george`?** `george` doesn't have all the functionality of `GPy`, but it is easy to use, and is faster than either of the other two. And I'm more familiar with it.\n",
    "\n",
    "We will also use another of Dan's packages, `emcee` to explore posterior probabilities using MCMC, and his `corner.py` module to plot the resulting parameter samples.\n",
    "\n",
    "Before you start, make sure you have the latest stable version of these packages installed. If you used `george` before, note the API has changed significantly between versions 0.2.x and 0.3.0.\n",
    "\n",
    "The easiest way to install all three packages is with `pip`:\n",
    "```\n",
    "pip install emcee\n",
    "pip install george\n",
    "pip install corner\n",
    "```\n",
    "\n",
    "Full documentation is available here:\n",
    "- https://george.readthedocs.io/\n",
    "- https://emcee.readthedocs.io/\n",
    "- https://corner.readthedocs.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import george, emcee, corner\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 0: working through the `george` introductory tutorials\n",
    "\n",
    "The `george` documentation includes some nice tutorials, which you'll need to run through before being able to tackle the problems below. Download and run the notebooks, making sure you understand what's going on at each step, and don't hesitate to ask questions!\n",
    "\n",
    "- [A gentle introduction to Gaussian Process regression](https://george.readthedocs.io/en/latest/_static/notebooks/first.ipynb): essentially does the same thing as problem 3 from Tutorial 1, but without a mean function. \n",
    "- [Model fitting with correlated noise](https://george.readthedocs.io/en/latest/_static/notebooks/model.ipynb): includes a mean function, and uses MCMC to explore the dependence of the posterior on the hyper-parameters. The same dataset is also analysed using a model with white noise only, to show how ignoring the correlations in the noise leads to over-confident estimates of the mean function parameters.\n",
    "\n",
    "Now you should have an idea of how to set up a basic GP model using `george`, how to make predictions, and how to evaluate the likelihood, optimize it, and explore the posterior using MCMC. I would also encourage you to try out the other tutorials, but they are not pre-requisites for this one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: A simple 2-D problem \n",
    "\n",
    "So far we have looked only at 1-D inputs, like time. Let's introduce a simple 2-d input case. We will generate some data using a 2-D polynomial and model it using a squared exponential GP.\n",
    "\n",
    "Run the cell below to generate and plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAEKCAYAAADq59mMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4b062bdfd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N = 100\n",
    "xobs = np.random.uniform(-5,5,N)\n",
    "yobs = np.random.uniform(-5,5,N)\n",
    "zobs = - 0.05 * xobs**2 + 0.03 * yobs**2 - 0.02 * xobs * yobs\n",
    "eobs = 0.01\n",
    "zobs += np.random.normal(0,eobs,len(xobs))\n",
    "plt.scatter(xobs, yobs, c=zobs, s=20, marker='.')\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$y$')\n",
    "cb = plt.colorbar()\n",
    "cb.set_label(r'$z$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1a: setting up the GP\n",
    "\n",
    "Now we will construct the GP model using `george`. We will use a with different length scales in each of the two dimensions. To set this up in george, you have to multiply two individual kernels together, like that:\n",
    "\n",
    "```\n",
    "k = a * KernelName(b, ndim = 2, axes = 0) * KernelName(c, ndim = 2, axes = 1)   \n",
    "```\n",
    "\n",
    "Here `KernelName` stands for the name of the kernel used (in `george`, the squared exponential kernel is called `ExpSquaredKernel`), `a` is the output variance, `b` is the metric, or length scale, applied to the first input dimension, and `c` to the second. \n",
    "\n",
    "Note this is equivalent to the parametrisation used in the lectures:\n",
    "$$\n",
    "k(x,x') = A \\exp \\left[ - \\Gamma (x-x')^2\\right] = A \\exp \\left[ - (x-x')^2/m^2\\right]\n",
    "$$\n",
    "with $\\Gamma=1/m^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go ahead and define the kernel in the cell below, with some ball park values for the hyper-parameters (by ball-park, I mean not too many orders of magnitudes off). Then create a GP object using that kernel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = 1.0 * george.kernels. # ...complete\n",
    "gp = george.GP(# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will need to tell the GP object what inputs the covariance matrix is to be evaluated at. This is done using the `compute` method. 2-D inputs need to be passed as an $N \\times 2$ array, which you will need to construct from the two 1-D arrays of $x$- and $y$-values we generated earlier. The second argument of `compute` should be the white noise standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xobs = np.concatenate([[xobs],[yobs]]).T\n",
    "gp.compute(# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1b: Optimizing the likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the example in the first `george` tutorial, define a simple neg log likelihood function, and a function to evaluate its gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neg_ln_like(p):\n",
    "    # complete\n",
    "    return # complete\n",
    "\n",
    "def grad_neg_ln_like(p):\n",
    "    # complete\n",
    "    return # complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the parameters which are accessed through the `set_parameter_vector` method are the logarithms of the values used in building the kernel. The optimization is thus done in terms of the log parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again following the same example, find the hyper-parameters that maximise the likelihood, using `scipy.optimize`'s `minimize` function, and print the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = minimize(# complete\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now assign those best-fit values to the parameter vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gp.set_parameter_vector(# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the predictive distribution\n",
    "\n",
    "Generate a grid of regularly spaced $x$ and $y$ locations, spanning the range of the observations, where we will evaluate the predictive distribution. Store these in 2-D arrays called `X2D` and `Y2D`. Then convert them into a single 2-D array of shape $N_{\\mathrm{pred}} \\times 2$, which will be passed to the GP's `predict` method.\n",
    "\n",
    "*Hint: use `numpy`'s `mrid` function.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X2D,Y2D = np.mgrid[# complete\n",
    "Xpred = np.concatenate(# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the best-fit hyper-parameters, evaluate the mean of the predictive distribution at the grid locations. The output will be a 1-D array, which you will need to reshape so it has the same shape as `X2D` and `Y2D` for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zpred = gp.predict(# complete\n",
    "Z2D = zpred.reshape(# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the cell below to plot contours of the  predictive mean alongside the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(xobs, yobs, c=zobs, s=20, marker='.')\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$y$')\n",
    "cb = plt.colorbar()\n",
    "cb.set_label(r'$z$');\n",
    "plt.contour(X2D,Y2D,Z2D);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualising the confidence intervals is a bit tricky in 3-D so we'll skip that. We could use `emcee` to explore the posterior distribution of the hyper-parameters, but we will leave that for a more realistic example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Separable functions\n",
    "\n",
    "In the above problem we were modelling a non-separable function of $x$ and $y$ (because of the cross-term in the polynomial). Now we will model a separable function, and use a GP with a sum rather than a product of kernels to separate the dependence on each of the input variable.\n",
    "\n",
    "This exploits the fact that GPs preserve additivity. In other words, a GP with a sum of kernels, each depending on a disjoint subset of the inputs, sets up a probability distribution over functions that are sums of functions of the individual subsets of inputs. This is how the K2SC pipeline (for removing pointing systematics in K2 data) discussed in the lectures works.\n",
    "\n",
    "As ever, we start by simulating a dataset. Execute the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 100\n",
    "xobs = np.random.uniform(-5,5,N)\n",
    "yobs = np.random.uniform(-5,5,N)\n",
    "zobs = -0.05 * xobs**2 + np.sin(yobs)\n",
    "eobs = 0.01\n",
    "zobs += np.random.normal(0,eobs,len(xobs))\n",
    "plt.scatter(xobs, yobs, c=zobs, s=20, marker='.')\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$y$')\n",
    "cb = plt.colorbar()\n",
    "cb.set_label(r'$z$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2a: Joint model\n",
    "\n",
    "We start, once again, by defining the GP object. The kernel will consist of a sum of 2 squared exponentials, one applied to each dimension. It will be useful to be able to access each of the kernel objects separately later, so start by defining each of the component kernel, assigning them to variables `k1` and `k2`, and then define the overal kernel `k` as the sum of the two. Then define the GP object itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k1 = # complete\n",
    "k2 = # complete\n",
    "k = # complete\n",
    "gp = # complete\n",
    "Xobs = # complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to optimize the likelihood. Luckily we can re-use the neg log likelihood and gradient functions from the previous problem. Start by packaging up the two inputs into a single 2-D vector, as in Problem 1, then use the `minimize` function to evaluate the max. likelihood hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gp.compute(# complete\n",
    "result = minimize(# complete\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the predictive distribution to check it worked ok. You can just copy and paste code from Problem 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# copy plotting commands from problem 1c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2b: Separating the components\n",
    "\n",
    "We now come to evaluating the predictive means for the individual components. The standard expression for the predictive mean is:\n",
    "$$\n",
    "\\overline{\\boldsymbol{y}}_* = K(\\boldsymbol{x}_*,\\boldsymbol{x}) K(\\boldsymbol{x},\\boldsymbol{x})^{-1} \\boldsymbol{y}\n",
    "$$\n",
    "The predictive mean for a given component of the kernel is obtained simply by replacing the first instance of the covariance matrix between test and training points, $K(\\boldsymbol{x}_*,\\boldsymbol{x})$, by the corresponding matrix for the component in question only:\n",
    "$$\n",
    "\\overline{\\boldsymbol{y}}_{1,*} = K_1(\\boldsymbol{x}_*,\\boldsymbol{x}) K(\\boldsymbol{x},\\boldsymbol{x})^{-1} \\boldsymbol{y}.\n",
    "$$\n",
    "\n",
    "`george` doesn't provide a built-in function to do this, but\n",
    "- the GP object has a method `apply_inverse`, which evaluates and returns the product $K(\\boldsymbol{x},\\boldsymbol{x})^{-1} \\boldsymbol{y}$ for a given vector of training set outputs $\\boldsymbol{y}$,\n",
    "- the kernel object has a method `get_value`, which evaluates the covariance matrix for a given set of inputs.\n",
    "\n",
    "Use these two functions to evaluate the two components of the best-fit GP model in our problem. Store the $x$- and $y$ components in variables `fx` and `fy`, respectively.\n",
    "\n",
    "*Hint: The `apply_inverse` method does what it says in the name, i.e. it modifies its argument by pre-multiplying it by the inverse of the covariance matrix. Therefore, you need to pass it a copy of the vector of obserced outputs, not the original.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = np.copy(zobs)\n",
    "gp.apply_inverse(#\n",
    "K1 = # complete\n",
    "fx = np.dot(# complete\n",
    "K2 = # complete\n",
    "fy = np.dot(# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now execute the cell below to plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(121)\n",
    "plt.plot(xobs,zobs,'.',c='grey')\n",
    "plt.plot(xobs,zobs-fy,'k.')\n",
    "s = np.argsort(xobs)\n",
    "plt.plot(xobs[s],fx[s],'r-')\n",
    "plt.subplot(122)\n",
    "plt.plot(yobs,zobs,'.',c='grey')\n",
    "plt.plot(yobs,zobs-fx,'k.')\n",
    "s = np.argsort(yobs)\n",
    "plt.plot(yobs[s],fy[s],'r-');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Multiple time-series with delays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a situation where we have several time-series, which we expect to display the same behaviour (up to observational noise), except for a time-delay. We don't know the form of the behaviour, but we want to measure the time-delay between each pair of time-series. Something like this might arise in [AGN reverberation mapping](https://en.wikipedia.org/wiki/Reverberation_mapping), for example.\n",
    "\n",
    "We can do this by modelling the time-series as observations of the same GP, with shifted inputs, and marginalising over the GP hyper-parameters to obtain posterior distribution over the time shifts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's simulate some data. We will cheat by doing this using a GP, so we know it will work. Execute the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 50\n",
    "M = 3\n",
    "t2d = np.tile(np.linspace(0,10,N),(M,1))\n",
    "for i in range(M):\n",
    "    t2d[i,:] += np.random.uniform(-5./N,5./N,N)\n",
    "delays_true = [-1.5,3]\n",
    "t_delayed = np.copy(t2d)\n",
    "for i in range(M-1):\n",
    "    t_delayed[i+1,:] = t2d[i,:] + delays_true[i]\n",
    "gp = george.GP(1.0 * george.kernels.Matern52Kernel(3.0))\n",
    "gppar_true = gp.get_parameter_vector()\n",
    "y2d = gp.sample(t_delayed.flatten()).reshape((M,N))\n",
    "wn = 0.1\n",
    "y2d += np.random.normal(0,wn,(M,N))\n",
    "for i in range(M):\n",
    "    plt.errorbar(t2d[i,:],y2d[i,:].flatten(),yerr=wn,capsize=0,fmt='.')\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3a: Initial guesses\n",
    "\n",
    "Because the function goes up an down, you can probably guess that the likelihood surface is going to be highly multi-modal. So it's important to have a decent initial guess for the time delays.\n",
    "\n",
    "A simple way to do obtain one would be by cross-correlation, but since the time-series are not regularly sampled (because of the small random term we added to each of the time arrays), we need to interpolate them onto a regular grid first. What better way to do this than with a GP? This will have the added advantage of giving us an initial estimate of the GP hyper-parameters too (we're assuming we don't know them either, though we will assume we know the white noise standard deviation).\n",
    "\n",
    "First we need to define a GP object, based on a Matern 3/2 kernel with variable input scale and variance. Do this in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = # complete\n",
    "gp = # complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to fit each time-series in turn, and compute the mean of the predictive distribution over a tightly sampled, regular grid of time values. If you take care to name our variables right, you can reuse the neg log likelihood and associated gradient functions from Problem 1.\n",
    "\n",
    "Complete the code below and run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p0 = gp.get_parameter_vector()\n",
    "# 2-D array to hold the best-fit GP HPs for each time-series\n",
    "p1 = np.tile(# complete\n",
    "# Regularly sampled time array\n",
    "treg = # complete\n",
    "# 2-D array to hold the interpolated time-series\n",
    "yreg = # complete\n",
    "c = ['r','g','b']\n",
    "for i in range(M):\n",
    "    # Compute the gp on the relevant subset of the 2-D time array t2d\n",
    "    # complete\n",
    "    # Assign the corresponding y values to the variable zobs\n",
    "    #Â (this is the one that neg_ln_like uses to condition the GP)\n",
    "    zobs = # complete\n",
    "    # Optimize the likelihood using minimize\n",
    "    result = minimize(# complete\n",
    "    # Save the best-fit GP HPs in p1\n",
    "    p1[i,:] = # complete\n",
    "    # update the GP parameter vector with the best fit values \n",
    "    gp.# complete\n",
    "    # evaluate the predictive mean conditioned on zobs at locations treg and save in yreg\n",
    "    yreg[i,:] = # complete\n",
    "    # you might want to plot the results to check it worked\n",
    "    plt.plot(t2d[i,:],y2d[i,:],'.',c=c[i])\n",
    "    plt.plot(treg,yreg[i,:],'-',c=c[i])\n",
    "# And let's print the GP HPs to see if they were sensible.\n",
    "print('Individual GP fits: best-fit HPs')\n",
    "print(p1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to cross-correlate the interpolated time-series. The easiest way to do this is using the function `xcorr` from `matplotlib.pyplot`. This function returns a tuple of 4 variables, the first two of which are the lags and corresponding cross-correlation values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt = treg[1] - treg[0]\n",
    "# Array to hold estimates of the time-delays\n",
    "delays_0 = np.zeros(M-1)\n",
    "for i in range(M-1):\n",
    "    # use pyplot's xcorr function to cross-correlate yreg[i+1] with yreg[0]\n",
    "    lags, corr, _, _ = # complete\n",
    "    # find the lag that maximises the CCF, convert it to time delay, save in delays_0 array\n",
    "    lmax = # complete\n",
    "    plt.axvline(lmax,color=c[i+1])\n",
    "    delays_0[i] = # complete\n",
    "plt.xlabel('lag')\n",
    "plt.ylabel('x-correlation');\n",
    "# Compare estimated to true delays\n",
    "print('Estimated time delays from cross-correlation')\n",
    "print(delays_0)\n",
    "print('True delays')\n",
    "print(delays_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the delays estimated in this way aren't too far off.\n",
    "\n",
    "To get initial guesses for the GP hyper-parameters, we can take the mean of the best-fit values from the three individual time-series. Do this in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gppar_0 = # complete\n",
    "print('Estimated GP HPs')\n",
    "print(gppar_0)\n",
    "print('True GP HPs')\n",
    "print(gppar_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GP HPs aren't too far off either.\n",
    "\n",
    "### Problem 3b: Optimization\n",
    "\n",
    "Now we have some initial guesses for the time-delays and the GP hyper-parameters, we're ready to model the time-series simultaneously, using a single GP. We need to write a new likelihood function to do this. The function will need to apply the delays to the times, before passing these times to `george` to evaluate the likelihood itself. \n",
    "\n",
    "First let's define a function `apply_delays`, which will take the delays and the time array `t` as inputs, and return an $M \\times N$ array of delayed times. This function will be called by the likelihood function, but it might be useful later for plotting the results too. It would also be useful for this function to warn us if the time-delays are such that one of the time-series no longer overlaps with the others at all, for example by returning a boolean variable that is true if all is well, but false if not.\n",
    "\n",
    "Complete the definition below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_delays(delays,t2d):\n",
    "    t_delayed = np.copy(t2d)\n",
    "    for i, delay in enumerate(delays):\n",
    "        t_delayed[i+1,:] # complete\n",
    "    ok = True\n",
    "    M = len(delays) + 1\n",
    "    for i in range(M):\n",
    "        # complete\n",
    "    return t_delayed, ok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to define the likelihood function itself. The likelihood should accept a parameter array consisting of the shifts first, and then the GP hyper-parameters, and make use of the output of `apply_delays` to return a very high number if the time delays are unreasonable. Complete the definition below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neg_ln_like_delays(p):\n",
    "    delays = p[# complete\n",
    "    t_delayed, ok = # complete\n",
    "    if not ok: \n",
    "        # complete\n",
    "    gp.set_parameter_vector(# complete\n",
    "    gp.compute(# complete\n",
    "    return # complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no simple analytical way to evaluate the gradient of the log likelihood with respect to the time delays, so we will not define a `grad_neg_log_like` function for this problem. The gradient descent optimizer will be slower, since it will have to evaluate the gradients numerically, but for such a small dataset it doesn't matter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now we are ready to run the optimizer. Like before, we can use the `minimize` function from `scipy.optimize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# concatenated array of true time delays and GP HPs\n",
    "ptrue = # complete\n",
    "# same for the initial guesses\n",
    "p0 = # complete\n",
    "print('Initial guesses')\n",
    "print(p0)\n",
    "result = minimize(# complete\n",
    "# save ML values in p1\n",
    "p1 = # complete\n",
    "print('ML parameters')\n",
    "print(p1)\n",
    "print('True parameters')\n",
    "print(ptrue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the optimization further improved our estimates of the time delays and the GP HPs. But how much can we trust these? Let's evaluate posterior uncertainties using MCMC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameter marginalisation.\n",
    "\n",
    "We now use MCMC to obtain uncertainty estimates, or confidence intervals, for the model hyper-parameters. \n",
    "\n",
    "First we need to define the posterior function to pass to the `emcee` sampler. We will use improper, flat priors over all the parameters, so the posterior probability is just a trivial wrapper around our `neg_ln_like_delays` function. Complete the definition below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lnprob(p):\n",
    "    # complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we set up the sampler. We will use 32 walkers, and initialise each set of walkers using the maximum likelihood estimates of the parameters plus a small random offset. Complete the code below, using the second `george` tutorial as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ndim, nwalkers = # complete\n",
    "p2 = # complete\n",
    "sampler = # complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to run the MCMC, starting with a burn-in chain of 500 steps, after which we reset the sampler, and run the sampler again for 100 iterations. Complete the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Running burn-in...\")\n",
    "p2, _, _ = # complete\n",
    "sampler.reset()\n",
    "\n",
    "print(\"Running production...\")\n",
    "sampler.# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we use the `corner` function from the `corner` module to plot the posterior distributions over the parameters. Complete the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = [r\"$\\Delta_1$\", r\"$\\Delta_2$\", r\"$\\ln A$\", r\"$\\ln\\l$\"]\n",
    "truths = ptrue\n",
    "corner.corner(# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully the distributions should look reasonable and be consistent with the true values. \n",
    "\n",
    "We need to extract confidence intervals for the parameters from the MCMC chain, which we can access through `sampler.flatchain`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "samples = # complete\n",
    "# The GP parameters were explored in log space, return them to linear space\n",
    "#samples[:, -2:] = np.exp(samples[:, -2:])\n",
    "# This handy bit of code will extract median and +/- 1 sigma intervals for each parameter\n",
    "pv = map(lambda v: (v[1], v[2]-v[1], v[1]-v[0]), zip(*np.percentile(samples, [16, 50, 84], axis=0)))\n",
    "# Print the results\n",
    "for i in range(ndim):\n",
    "    pval = # complete\n",
    "    print(\"Param {}: {:5.2f} +{:4.2f} -{:4.2f} (true: {:5.2f})\".format(i+1,pval[0], pval[1], pval[2], ptrue[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, the MCMC estimates should be consistent with the true values..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge problem: Active scheduling\n",
    "\n",
    "Imagine you are monitoring a particular variable, you want to know its value to a given precision at anyone time, but each observation is costly, so you don't want to take any more than you have to. You can train a GP on the first few observations, then use the predictive distribution to work out when your uncertainty about the current value of the variable is so large that you need to take a new observation. Use the new observation to update the GP hyper parameters and the predictive distribution, and repeat the process...\n",
    "\n",
    "First we generate a tightly sampled time series over 100 days. This will represent the \"true\" value of the variable. We will include some periodic behaviour as that makes the problem more interesting. Then we will \"observe\" 1 point per day for the first 20 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtrue = np.linspace(0,100,1000)\n",
    "k = george.kernels.CosineKernel(np.log(12.3)) * george.kernels.ExpSquaredKernel(1000.0)\n",
    "ytrue = george.GP(k).sample(xtrue)\n",
    "xobs = xtrue[:200:10]\n",
    "eobs = 10.0**(np.random.uniform(-1.5,-1,20))\n",
    "yobs = ytrue[:200:10] + np.random.normal(0,1,20) * eobs\n",
    "plt.plot(xtrue,ytrue)\n",
    "plt.errorbar(xobs,yobs,yerr=eobs,fmt='.',capsize=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to devise and implement an algorithm that will schedule observations, based on the data to date, so as to ensure the uncertainty on the value of the function at any one time never exceeds 0.1. At each step, the aglorithm should:\n",
    "- train a GP on the data acquired so far. You may assume the form of the covariance function is known, as is the output variance, so there are only two hyper-parameters to fit (the log period of the cosine kernel and the metric of the squared exponential term).\n",
    "- make predictions for future values. If you're being clever, you can do this sequentially so you only look ahead a small time interval at a time, and stop as soon as the uncertainty exceeds the desired bound.\n",
    "- use this to decide when to take the next observation\n",
    "- add the next observation (by sampling the \"true\" values at the appropriate time and adding noise with the same distribution as above)\n",
    "- repeat till the end time is reached.\n",
    "\n",
    "Of course you will need to test your algorithm by comparing the predictions to the true values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gp = george.GP(k)\n",
    "gp.set_parameter_vector([np.log(10),np.log(1000)])\n",
    "gp.compute(xobs,yerr=eobs)\n",
    "def nll(p):\n",
    "    gp.set_parameter_vector(p)\n",
    "    return -gp.log_likelihood(yobs)\n",
    "def gnll(p):\n",
    "    gp.set_parameter_vector(p)\n",
    "    return -gp.grad_log_likelihood(yobs)\n",
    "result = minimize(nll, gp.get_parameter_vector(), jac=gnll)\n",
    "print(result)\n",
    "gp.set_parameter_vector(result.x)\n",
    "ypred, epred = gp.predict(yobs, xtrue, return_var=True)\n",
    "plt.plot(xtrue,ytrue)\n",
    "plt.errorbar(xobs,yobs,yerr=eobs,fmt='.',capsize=0);\n",
    "plt.fill_between(xtrue,ypred + epred, ypred-epred,alpha=0.2,edgecolor='none')\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
